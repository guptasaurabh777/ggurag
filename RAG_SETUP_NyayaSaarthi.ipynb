{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RAG SETUP - NyayaSarthi"
      ],
      "metadata": {
        "id": "AoKiLKcI3QoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "UNmZ5fHr3ZYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-openai faiss-cpu tiktoken pypdf ragas sentence_transformers"
      ],
      "metadata": {
        "id": "dK09olZm3PIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Packages"
      ],
      "metadata": {
        "id": "o-EtVf4J3h-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from ragas.metrics import LLMContextRecall\n",
        "from ragas import EvaluationDataset\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall\n",
        "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
        "from ragas.metrics import ResponseRelevancy\n",
        "from ragas.metrics import Faithfulness\n",
        "from ragas import evaluate\n",
        "import os"
      ],
      "metadata": {
        "id": "dbVDlBKe3mHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Azure Opneai Keys & Endpoint"
      ],
      "metadata": {
        "id": "YrltUzDL4Ef7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\""
      ],
      "metadata": {
        "id": "0Pc2zmxj4Iq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup LLM with required parameters like temperature , top_p, max_tokens etc"
      ],
      "metadata": {
        "id": "-ViWZlEV4Ln0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(\n",
        "    openai_api_version=\"2025-01-01-preview\",\n",
        "    azure_deployment=\"gpt-4.1-mini\",  # Replace with your deployment name\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_tokens=2000)"
      ],
      "metadata": {
        "id": "DvbQZ4k74QLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant\",\n",
        "    ),\n",
        "    (\"human\", \"what is capital of india?\"),\n",
        "]\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "id": "NTF0iGWd4TzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build FAISS Vector Index by loading the PDF & chunking , Embedding them"
      ],
      "metadata": {
        "id": "kppleAWW4qXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uplaod the 3 PDF\n",
        "### 1. BNS.pdf\n",
        "### 2. BNSS.pdf\n",
        "### 3. BNSA.pdf"
      ],
      "metadata": {
        "id": "Mkoho_jG4tBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_path = \"faiss_index\"\n",
        "\n",
        "if os.path.exists(vectorstore_path):\n",
        "    print(\"ðŸ”„ Loading existing FAISS index...\")\n",
        "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = FAISS.load_local(vectorstore_path, embeddings=embed_model, allow_dangerous_deserialization=True)\n",
        "else:\n",
        "    print(\"ðŸ†• Creating new FAISS index from PDFs...\")\n",
        "    loader = DirectoryLoader(\"data/\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    docs = loader.load()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_documents(chunks, embed_model)\n",
        "\n",
        "    db.save_local(vectorstore_path)"
      ],
      "metadata": {
        "id": "FtPV2-wy4pnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriveal Augment QA"
      ],
      "metadata": {
        "id": "klnlo--Q468W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a legal assistant. Use the below excerpts from law documents to answer the question.\n",
        "Mention source and page number after each answer fragment.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Helpful Answer with citations:\n",
        "\"\"\")\n",
        "\n",
        "stuff_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, stuff_chain)"
      ],
      "metadata": {
        "id": "l_n3sKno49sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask Question & Get Answer"
      ],
      "metadata": {
        "id": "BHYCpJHT5Xuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"What is the section no that deals with  theft and punishment duration for the same  under the new law BNS?\"})\n",
        "print(\"\\nðŸ“˜ Answer:\\n\", response[\"answer\"])"
      ],
      "metadata": {
        "id": "vfE1iEbY5bBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Results using a pre-curated Synthetic Golden Dataset"
      ],
      "metadata": {
        "id": "_KtRRUK15iUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_QUESTIONS=[\n",
        "    \"What are the section that deal with bribery?\",\n",
        "    \"What is the punishemnt for kidnapping or abducting in order to murder or for ransom?\",\n",
        "    \"What is the section that deals with culpable homicide?\",\n",
        "    \"What is the section that deals with donwry?\"\n",
        "]\n",
        "\n",
        "EVAL_ANSWERS=[\n",
        "    \"Section 170 deals with bribery, diefining it is a giving or accepting gratification to induce someone to exercise electoral rights.Section 173 addresses punishment for bribery, stating it shall be punished for fine only.\",\n",
        "    \"For fidnapping or abducting in order to murder the punishment is imprisonment for life or rigorous imprisonment for 10 years and fine.For kidnapping for ransome the punishment is death or imprisonment for life and fine. Both offenses are cognizable and tried by the cour of session\",\n",
        "    \"Section 100 deals with culpable homicide.Section 101 defines murder, while section 105 addresses punishment for culpable homicide not amounting to murder, Section 102 covers coulpable homicide by causing the death of a person other than the intended victim\",\n",
        "    \"Section 80 deals with dowry death. It defines dowry death as the death of a woman with seven years of marriage caused by burns, bodily injury or under abnormal circumstances where she was subjected to cruelty or harassement related to downry demands\"\n",
        "]\n",
        "dataset =[]\n",
        "\n",
        "for query, reference in zip(EVAL_QUESTIONS, EVAL_ANSWERS):\n",
        "  relevant_docs = retriever.invoke(query)\n",
        "  response = rag_chain.invoke({\"input\": query})\n",
        "  dataset.append({\"user_input\": query,\n",
        "                  \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "                  \"reference_answer\": reference,\n",
        "                  \"response\": response[\"answer\"],\n",
        "                  \"reference\":reference\n",
        "                  })"
      ],
      "metadata": {
        "id": "qDLgBC0d5lvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset = EvaluationDataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "lxT7cCXJ5r0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(llm)"
      ],
      "metadata": {
        "id": "3941AmEE5sfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate(dataset =evaluation_dataset,\n",
        "                  metrics =[LLMContextRecall(),Faithfulness(),ResponseRelevancy(),LLMContextPrecisionWithoutReference()],\n",
        "                  llm = evaluator_llm, embeddings= embed_model )"
      ],
      "metadata": {
        "id": "plXfx_Qp5vSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "iMjOCrV65x09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}